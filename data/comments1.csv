author,updated_at,like_count,text,public
@bigbluetunafish4997,2023-11-19T10:32:07Z,2,"finally i finished these 4 chapters of neural networks, and some of your linear algebra and calculus stuff. i feel much better that now i have deeper understanding of how neural network works and have built up that base for further exploration of machine learning.  thanks very much for your effort creating all these great videos together.",True
@michaelzauzig5592,2023-09-29T19:09:55Z,2,these videos are just superb. thanks so much for making them.,True
@alexdebate7081,2023-09-15T20:37:12Z,9,"grant, i've come back to this series many times over the last five years. every time i do, i pick up more and more pieces of the puzzle. i think i've finally got it now, but we shall see! thank you!",True
@securemax,2023-08-29T19:11:03Z,3,"for everyone wanting to implement backprop from scratch: don't use dc/da = 2*(a-y). instead use dc/da = a-y. this is because the cost function would actually be defined with a factor 1/2 in front which is missing here. hence, the derivative changes. all other derivatives are good :)",True
@meeradad,2023-08-05T17:38:45Z,2,these videos are the best ways to make a high schooler fall in love with calculus instead of hating it or fearing it. and open his/her mind to the joy of creativity rooted in mathematical insights.,True
@yashjindal9822,2023-06-21T00:19:10Z,50,i just started out with my ml career. this entire series made me feel as if i knew it all along. thank you grant i will return to this comment to share my professional progressðŸ˜Š,True
@thomasclark8922,2023-04-15T23:10:52Z,1115,"this series was my first introduction to machine learning 3 years ago. i now work full-time as an aiml scientist, my life is forever changed. thank you.",True
@SaintKhaled,2023-04-03T06:44:17Z,76,the quality of this education is top-tier. i absolutely am speechless that you make it freely accessible. thank you so much!,True
@tooljerk666,2022-12-17T01:52:31Z,5,"such a great video. however, i'm still a little unclear why z(l) / x(l) is just a(l-1)? is it because wl) and b(l) are constants?",True
@hutc22222222,2022-12-14T06:19:14Z,179,"your work of making high levels of math accessible to anyone wishing to learn a variety of new topics is not obvious to me. you succeed to explain everything so clearly, making me want to start learning maths again, reminding me of and introducing me to beautiful aspects of math, and you deserve more than a 'thank you' :)",True
@thomasschwarz1973,2022-12-12T15:55:10Z,5,"this is truly awesome, as pedegogy and as math and as programming and as machine learning.   thank you!   ...one comment about layers, key in your presentation is the one-neuron-per-layer, four layers.  and key in the whole idea of the greater description of the ratio of cost-to-weight/cost-to-bias analysis, is your l notation (layer) and l - 1 notation.   problem, your right most neuron is output layer or ""y"" in your notation.   so one clean up in the desction is to make some decisions: the right most layer is y the output (no l value), because c[0]/a[l] equals 2(a[l] - y).   so the right most three neurons, from right to left, should be y (output), then l then l minus one, then all the math works.  yes?",True
@kirilllosik7054,2022-12-03T08:52:22Z,7,"thanks a lot for creating such a fantastic content! anticipating to see more videos about ai, ml, deep learning!",True
@elizfikret7489,2022-10-10T08:59:53Z,16,thank you so much! i have understood more math from this channel than from all teachers i have had in high school or university in total.,True
@vladimirfokow6420,2022-06-21T20:21:03Z,169,"thank you a lot for this series! it has really helped me get into this topic, and changed my life. your intuitions have been immensely helpful in my efforts to understand backpropagation. i just can't overestimate, how great your channel is!",True
@bilalsedef9545,2022-03-11T19:24:20Z,3,this is a great and very educational video. but i think it needs one more part to show how the weights are updated.,True
@snf303,2022-02-20T18:58:06Z,54,"at time when i just finished my university â€” i could not imagine that at one chilly sunday evening, in almost 15 years after the graduation, i will sit with a bottle of beer, watch math videos, and have so much fun! thank you!",True
@bean217,2021-11-14T22:03:34Z,2,"i am currently going through michael nielson's ""neural networks and deep learning"" book. this video helps to clear up and visualize the chapter on back propagation a lot. thank you for making this video series.",True
@alpinstef3566,2021-11-03T23:27:54Z,4,"great presentation! very helpful. although, could you clarify if the indices at 9:30 are consistent? if you sum over j in the 'next' layer, you would have k in the current layer. instead of w_jk^(l+1), would it be w_ij^(l+1) and sum over i for this next layer? thanks!",True
@ItatiaiaBR,2021-09-01T11:14:37Z,2,that's the end? wtf,True
@ganjarulez009,2021-07-16T10:53:14Z,2,"i don't understand why the books cant explain this stuff this easy yet precise, great job!",True
@antovrdoljak1317,2021-05-29T07:50:09Z,2,it really can`t get any better than this.  awesome! this is truly the peak of learning methodology and didactics!,True
@LinsuHan,2021-05-04T21:58:40Z,6,"it's good to visual the vanishing gradient problem here at 4:30 : - if the activation is sigmoid, the derivative is ranges between [0, 0.25] - if the activation is relu, the derivative is either 1, or 0 with enough sigmoid activations stacked on top of each other you can see that the upper limit of gradients approaches (.25)^n -> 0",True
@programmingsoftwaredesign7887,2021-04-11T09:07:45Z,2,"""all of these are just numbers right?"" -3b1b no, this is madness!",True
@zilongzhao3274,2021-03-19T01:31:49Z,3,"your video should be shown in every university's lesson, the animation makes the calculation just so easy to understand.",True
@kaichristensen6723,2021-02-03T05:33:22Z,169,"i'm taking machine learning by andrew ng on coursera right now, and just got stuck on backpropagation. thank you thank you thank you thank you grant, you have no idea how incredibly helpful your videos are and how much your channel has inspired me through the years.",True
@vedant7090,2021-01-30T12:33:31Z,25,man u deserve a nobel prize for teaching machine learning with this simplicity.,True
@vishnukrishnaprasad424,2020-12-27T04:58:53Z,2,"i was struggling to wrap my mind around the math of this. beautifully explained, thank you <3 subscribed!",True
@colorlace,2020-12-09T02:50:42Z,2,best explanation i've ever seen of the chain rule in a video that isn't about the chain rule,True
@tiagol8200,2020-10-11T19:43:19Z,2,i'm proud of myself for understanding most of that. great video!,True
@matthewhaythornthwaite754,2020-08-22T15:15:05Z,32,"if anyone is interested, i worked through the chain rule for the differential of the cost function w.r.t the weight in the second layer down. two additional terms are added to make everything cancel as they should. it shows how as you progress down the layers, more partial differentials are added to the equation from all the variables above, making it more unstable and hence more susceptible to the exploding or vanishing gradient problem.   dc/dw(l-1) = dz(l-1)/dw(l-1) * da(l-1)/dz(l-1) * dz(l)/da(l-1) * da(l)/dz(l) * dc/da(l)",True
@danielmorgan6540,2020-07-04T20:20:07Z,2,"at 4:30, how do you calculate the derivative of sigmoid(z^l)?",True
@sainandandesetti3268,2020-06-08T04:13:19Z,5,"stunningly beautiful... the best part of the series (for me, obviously) is that the beauty of this series does not make it very easy to understand. no. each video may need multiple views. but these videos are so beautifully made that you'd want to watch them again and again, not with the frustration of getting your head over a concept but with the thrill of unravelling a mystery...  so for creating such excitement in me, thank you.",True
@bradleydennis210,2020-05-31T04:48:24Z,40,i just finished up calc iii this semester and i have never felt happier with myself for being able to apply my new knowledge than this episode. i also don't think i have ever been more excited to hear calc iii topics being brought up in a field i am trying to teach myself currently. thank you for making such a simple to understand series!,True
@rohitdatla724,2020-05-17T18:48:36Z,2,"u r not just teaching nn concept but how to think, break down and understand any complex problem and digest, u r awesome!!!!!",True
@Kevin-cy2dr,2020-05-06T10:09:10Z,2,"honestly this channel doesn't deserve a dislike button. it took me days to figure out one video(at the beginning),but the concepts remain still in my head. this channel taught us that maths is not just changing numbers, but its conceptual and intuitive just like science. grant if you are ever read this, please know that you are one of the very few people that change the world. i just dont have words for you man, great job is an understatement for you. i promise once i earn enough i will contribute to your channel",True
@thiyagutenysen8058,2020-05-02T17:26:20Z,287,i came here after andrew ng's week 5 in coursera and you blew my mind,True
@wattsfield1889,2020-02-18T17:45:03Z,4,"you're videos are of mad quality! i'm upgrading my debit card to credit so i can become a patreon, cuz damn. i think we often take amazing youtube videos for granted. creators like 3b1b give us so much. it's only right to give back a little sometimes.",True
@samozturk8276,2020-01-26T11:13:27Z,2,"hey, first of all your videos are great. it makes learning process extremely easy and fast.    quick question: i was wondering how you make those animations? i need to make presentations constantly at work, so it would be easier if i can explain things as you do to a non-techinal person(in this case my manager).",True
@ShyamSunder-se3tq,2020-01-19T17:57:39Z,2,"if i get anywhere in this field, you'll have the credit for actually starting me up...",True
@LimitedWard,2020-01-10T06:00:44Z,21,"absolutely brilliant explanation! i took a course on deep learning in college, but ended up auditing it in the end because i couldn't grasp the concepts well enough to pass the tests. you just took the entire first unit of the course, which took several weeks, and condensed it into 4 easily digestible videos that anyone can understand!",True
@sharkk2979,2020-01-07T11:00:08Z,3,thanks lord for the info à¤…à¤¦à¥à¤µà¤¿à¤¤à¥€à¤¯!!!!,True
@mooglefan,2019-12-31T13:59:04Z,12,i've worked with ai for 2 years now. i have never seen anyone explain this as succinctly and aptly as you have. this video is legitimate gold. going to show this to anyone who needs an explanation in future!,True
@mike_o7874,2019-12-29T13:20:25Z,5,"i watched this video like 10 times at least and, finally! finally! i was able to build my own neural network with back prop, in c# and it actually works!  btw, i found out that if my layers have a lot of neurons the system often over shoting, and deviding the cost for each neuron by how many neurons, are in that layer help to fix that issue, i mean i can have the same training value for any type of neural network and it wont over shoot at all. but in the video you just said to add the costs for each neuron together rather then getting the average cost for each neuron....  hmm...",True
@qwert-cj4ld,2019-12-02T14:25:53Z,13,9:28 sums up the whole thing,True
@jarjuicemachine,2019-11-06T15:55:55Z,4,number of times i have watched this video before understanding â†“,True
@noahkupinsky1418,2019-09-21T06:50:51Z,900,hey for all of you getting discouraged because you donâ€™t understand this - that was me last year. i went and taught myself derivatives and came back to try again and suddenly i understand everything. itâ€™s such an amazing feeling to see that kind of work pay off. donâ€™t give up kiddos,True
@nepali1651,2019-09-17T12:39:45Z,2,"just about an hour ago, i was totally alien to ai guys especially when they said ""machine learns"". hats off to your selflessness making even a medico able to understand how a machine actually learns, relatively easier when i compared it to natural neural networks in our nervous system. mark my words, with raging ai in healthcare, your videos will be a connecting link for someone away from ai, to know about how ai works.",True
@Mrrajender2801,2019-09-11T16:01:25Z,86,many guys claim to know. some guys actually know. but only one guy actually knows and can explain to his grandma as well with very beautiful animations. you are that one !!!,True
@mukundholo6019,2019-07-10T06:14:04Z,2,"this is math at the best and art too at the highest. the grace of the animation, the subtle music, the perfectly paced narration and the wonderful colour scheme! math and art or let's say math is art!",True
@prashamsht,2019-07-07T04:30:10Z,6,"one of the best lectures i have ever heard. great explanation of nn, cost functions, activation functions etc. now i understand nn far far better...(p.s. i saw previous videos part 1, 2,3 as well)",True
@samarthsingla1082,2019-07-05T01:03:39Z,8,the amount of help you are providing is nothing short of amazing.,True
@srijan4622,2019-07-01T03:35:07Z,2,"so, i get that the desired output(y) for the layer of neurons in the output layer can either be a 0 or a 1. but, what is the desired output(y) when calculating the gradients for the second to last layer of neurons? what activation do we actually desire for the layer behind the activation layer?",True
@SaifUlIslam-db1nu,2019-06-28T17:13:35Z,290,"it has taken me about 3-4 days worth time to understand all of these 4 lectures, lectures which are in total, no longer than 1 hour and 30 minutes.     and i feel proud.",True
@theodorechandra8450,2019-06-20T09:24:29Z,2,i should put your name in the acknowledgement part of my thesis,True
@PowerOfTheMirror,2019-06-09T08:51:12Z,5,how do you determine the desired value of a neuron in the hidden layer?,True
@1nicevids,2019-05-12T05:35:37Z,2,"i noticed in one of the previous videos, you showed that this was just the beginning of a series of neural network topics, with convolutional neural networks and lstm listed down the path.   do you plan on making a future video series on more specific types of neural networks like cnns and lstm?",True
@nairanvac79,2019-04-05T12:09:46Z,38,thank you for starting your indices at 0.,True
@sacation6057,2019-03-16T11:26:54Z,4,"awesome series! even though i already had quite a intuitive feeling about the concepts of deep learning, your videos just always make complex subjects click in my mind, it sort of forms the right connections between the neurons in my mind i suppose so ;) even without any advanced math knowledge i was able to follow your math, so thanks for choosing to keep your examples as simple as possible!  i'm gonna make my own network from scratch in code some time, to see if i truly understand it throughly.",True
@BrutalGames2013,2019-01-01T19:42:54Z,2,you deserve an oscar in visual story telling! thank you for your work!,True
@user-cl5mm3be7m,2018-11-18T04:55:42Z,5,çœŸæ˜¯å¤§å¸«ç´šçš„ä½œå“ï¼Œè§£é‡‹å¾—éžå¸¸æ¸…æ¥šï¼Œå¤ªç¥žå¥‡äº†! awesome!!,True
@samuelreed5481,2018-10-14T19:15:12Z,19,these videos are unbelievably well produced. thank you so much for your effort. you've made this topic incredibly clear and i cannot understate how much i appreciate the amount of effort you put into these. you have incredible talent as a teacher.,True
@tcc1234,2018-09-27T14:11:31Z,3,i saw a 6.5 minute ad (related to ai) in order to support you. keep up!,True
@Redrumy0,2018-09-19T22:19:34Z,20,"literally the only youtube channel, that makes studying 2 hours of math, go by in a blink of an eye <3 thank you so much :d",True
@vectozavr,2018-07-29T15:56:37Z,325,that is the reason for learning the math! to understand such a beautiful things! that is awesome! thank's a lot!!!,True
@cowcannon8883,2018-05-24T21:48:41Z,666,"neural networks have layers, ogres have layers shrek is an ai confirmed",True
@vasishtapolisetty639,2018-05-06T14:14:48Z,2,i am a medstudent who left math back in class 10. the fact that i could at least remotely follow till the end of this video series  just shows how you have tremendous pedagogic skill. great work!,True
@snippletrap,2018-04-08T21:02:19Z,2,please continue this series. i want to see how we get features like loops and edges.,True
@shofada,2018-03-08T17:06:41Z,302,"this is how 21st teaching should look like. it feels like your work should be made a ""human right"". thank you.",True
@AnshulKanakia,2018-01-09T21:30:44Z,4,"i can't tell you how long i've been trying to visualize all this in my head to get a solid mental picture of backpropagation... well, i guess i can - it was the duration of a flight from frankfurt to seattle (about 9 hours) and it involved one terribly lit backside of an airplane menu and a shitty pencil. i am so grateful for the work you put into animating this algorithm. it has literally brought tears to my eyes and a smile on my face. thank you.",True
@DennisAprillaChristie,2017-12-27T11:18:34Z,2,best. explanation. ever.,True
@ParadoxCircuit,2017-11-15T01:41:54Z,2,"damn i was going so strong for the first three videos, but this one has me thinking i'm going to have to rewatch it 3 or 4 times to actually grasp the significance.",True
@defy7692,2017-11-11T13:09:06Z,9,"that feeling when you understand completely without even pausing one time :) i would love to have you as my advanced analysis teacher, your explanations are better than great !",True
@youngnam1175,2017-11-11T12:50:46Z,6,"thanks 3b1b. i'm understanding machine learning mush better, and following your video while note taking was the easiest method for learning.  i'm a little confused about what the change in c_0 with respect to change in a_(l-1, k) for the k-th activation in l-1 layer (i just changed to this notation because i feel more comfortable writing like this in one line text). that's 8:40 part of the video i guess. it doesn't make intuitive sense for me as to why you need the summation of impact of a_(l-1, k) on a_(l, 0~n), say without any multiplier or something.  trying to understand the meaning of `dc_0/da_(l-1, k)` i thought of a neural network where there are only two layers, input and output layer, and input layer having 1 neuron and output layer having 2 neurons.  does it ever make sense for a_(l-1, k) to be an activation (or neuron?) in an input layer? if so, i think it makes to add the 'impact' all up especially when the weights are all same 'direction' or sign because if so summing them all up would result in greater number, and this would mean changing the input has the biggest impact in this scenario.  if not, i'm still confused what `dc_0/da_(l-1, k)` is and why it has the summation.",True
@alexandrugheorghe5610,2017-11-11T11:45:49Z,3,excellent lesson! thank you very much.,True
@thfreakinacage,2017-11-08T10:23:17Z,45,"my god! a basic machine learning video series that actually makes sense to completely beginners! subscribed, and waiting in great anticipation for the next one! :d",True
@borg286,2017-11-08T09:13:54Z,52,"the point where you addressed the concern that the example you were using was too simple, having only 1 edge, was spot on as you were leading me down this merry garden path. i appreciate how much you watch your own videos and predict where the watcher would mentally say, ""but what about...""",True
@jaysoaring6318,2017-11-06T18:14:26Z,8,if there is an award for educational video series on advanced scientific matters. please give this award to 3b1b. love it!,True
@micahsheller101,2017-11-06T17:41:03Z,4,"beautiful work! reminds me of my late father who was a math professor: he had the same gentle, happy style, and believed heartily in making math a safe place for everyone to learn and have fun. gonna make me tear up :)",True
@antopolskiy,2017-11-06T17:30:08Z,3,this is sooooo useful. thank you so much for breaking it down so great. i feel like i have a full picture in my head now.,True
@Jabrils,2017-11-06T12:27:51Z,748,youre a deity grant,True
@ehsanmon,2017-11-05T21:49:00Z,6,"thank you so much, grant. i finally learned back prop, and i have become a patron. i wish i could do more.",True
@kimtheass1,2017-11-05T13:42:12Z,3,you videos have such informative and well explained content with an amazingly calm and including tone! i'm a fan,True
@GaborGyebnar,2017-11-05T13:13:55Z,80,"awesome material. :) please make a video about convolutional neural networks, too.",True
@PhilippeCarphin,2017-11-04T21:25:17Z,378,"this series is insanely good.  as a teacher, i feel like salieri watching mozart play and being like ""it's so beautiful, how is he so good!""",True
@NicolasSchmidMusic,2017-11-04T15:45:05Z,2,"please keep on doing videos about that topic, youâ€˜ve realy opened my eyes on an amazing univer which is the neural network and deep learning",True
@Erioch,2017-11-04T13:42:44Z,4,"honestly, this is one of the best (if not the best) channel on mathematics/science education i have seen. intuitive but not oversimplified. thank you so much that for offering your spectacular work and you help so many people understand these concepts.",True
@annemarieenpieterbresters-3876,2017-11-04T11:19:24Z,3,"this must have taken so much time, i was able to just understand it now (after using my whole evening yesterday)",True
@nagoshi01,2017-11-04T09:49:38Z,4,what function is he saying at 2:00? he says sigmoid and then something that i don't know how to spell to look up,True
@JM-us3fr,2017-11-04T04:39:26Z,2,i can't believe i subscribed to you 2 years ago!  (with a different account) and i've never regretted it :d,True
@jamiepeters846,2017-11-04T03:45:09Z,2,you truly have a gift for teaching in a clear and concise manor! thanks for the great vid!,True
@TheZenytram,2017-11-04T02:14:14Z,4,wow i thought that the math behind it would be waaayy more complicated. i know that this video has a lot information to digest but it's not complicated.,True
@sergiokorochinsky49,2017-11-04T02:03:59Z,266,"i just unsubscribed to this channel, so i can have the pleasure of subscribing again.",True
@kangChihLun,2017-11-03T23:19:42Z,298,this is the best and clearest explanation in all bp course i could find !  æ²’æœ‰ä¹‹ä¸€ï¼,True
@NitinNataraj-gf3vx,2017-11-03T22:54:27Z,252,netflix can show these rather than some other questionable material.,True
@giron716,2017-11-03T22:52:16Z,6,"i seriously have a hard time explaining how much i appreciate this video. i am far and away a symbolic thinker, as opposed to a geometric one, and while i love all of your videos and how intuitive you make the concepts, it's sometimes hard for me to think about the geometry. i am much more comfortable working with symbols and that's why i treasure videos like this. thank you :)",True
@AbhishekKumar-bo1yi,2017-11-03T22:21:46Z,8,"i always feel, if u have a mentor who can break complex things into simple stuff so beautifully, even a dumb guy can grasp the concept. keep doing the good stuff. admirer++",True
@jeromebruzaud3995,2017-11-03T20:53:28Z,3,awesome video as always. i'm actually a bit sad it marks the end of your video series on deep learning :(. give us more ! :d,True
@MeriaDuck,2017-11-03T20:04:03Z,3,"after seeing a few pieces of books, descriptions on the internet about back propagation, with this video i finally reached some kind of enlightenment (especially at about 4:50 into this video). thank you so much for that!  just as a hobby, i was trying to implement a neural network from scratch in java: plain objects for neurons, connections and layers.  i wanted to really visualize how a neural network works. (visualize either as computer code, but maybe i even want to create some visual for it...) this will certainly help me on my way!",True
@jannegrey593,2017-11-03T19:41:39Z,2,"i'm going to be frank. i love your videos, the're simple and understandable. but with neural network i feel like a moron. still, thank you for those videos, and i hope that in a couple of weeks there will be another one.",True
@iau,2017-11-03T18:28:07Z,36,"is it wrong to use a programming-like notation for vectors and matrices (a[l, n], or for example a[3, 5] for the activation of neuron 5 of layer 3) inside mathematical notation, instead of using subscripts and superscripts?  i ask because i usually get confused thinking superscript indices are exponents and subscript indices are variations or alternatives, instead of the next one in a sequence.  i ask because i find the notation x[t] as the t-th element of sequence x much more intuitive than x_t.",True
@hiqwertyhi,2017-11-03T18:10:56Z,887,"it's not that no-one else makes top-notch math/cs videos, it's that this guy makes it click.",True
@Ensorcle,2017-11-03T18:07:43Z,69,i cannot tell you how much i appreciate these videos. i don't have a strong math background (english undergrad) but i'm teaching myself data science. it really helps to have the equations explained rather than just presented and to tie the components of the equation back to intuitions. thank you thank you thank you.,True
@Abstruct,2017-11-03T17:37:14Z,51,"this stuff is an amazing supplication to andrew ng's courses, it gives a lot more intuition and visual understandings of the formulas.",True
@rahul7270,2017-11-03T17:08:33Z,2,nothing's better than 3b1b releasing two videos at once. yay!,True
@borismilenski4759,2017-11-03T16:27:54Z,2,i'm so glad i took an extra course in math  which gave me the basics of calculus and pointed me to this channel. it makes sense!,True
@suharsh96,2017-11-03T16:23:25Z,380,"this is the longest 10 minute video i have ever watched. literally took me half an hour, but the feeling of the idea behind this completely settling in , makes it totally worth it!",True
@4AneR,2017-11-03T16:13:19Z,82,"what an art of math, jesus christ",True
@aravindkannan9490,2017-11-03T15:39:27Z,26,this is by far the best video i have ever seen in neural networks. thanks for this! :),True
@antoniobernardo9884,2017-11-03T15:31:44Z,531,this is easily the best channel in youtube today! once i get a job i will more than glad to support you!,True
@sjgmc,2017-11-03T15:23:57Z,6,"as an hobbyist programmer, i can't thank you enough! once i finish my studies i will donate to you. :)",True
@notbobbobby,2017-11-03T15:23:20Z,7,"right now, i am so thankful for having taken vector calculus and several numerical approximation courses. this was an awesome video to watch. thanks! :)",True
@vadimborisov4824,2017-11-03T15:13:48Z,3,thank you for the appendix!!!!!!!!!,True
@lagduck2209,2017-11-03T14:58:56Z,194,"*looking at thumbnail oh sh~, im never going to understand that complex stuff. probably should watch it anyway *ten minutes later whoa! that's actually quite clear now!",True
@saptarshimitra1267,2017-11-03T14:54:25Z,620,amazing man..... i say 3gold1platinum,True
@poundcake2000,2017-11-03T14:51:52Z,106,"grant, i've been thinking about these videos and i have a couple questions.... last time you fed a nonsense, scrambled image through the network, and it confidently identified it as a 5.  my question is two fold:  1) would any value be added to the network by adding a final neuron that indicates ""no number"" or ""not anything identifiable"" or ""throw this image in the trash can""?  maybe that way the network would sort those trash-type images into a pile for, perhaps, a human to look at.  2) is there any benefit to training your network to get the right answer, but be less confident about it?  you keep mentioning that we want the network to identify a ""2"" as a ""2"" and the goal is a final neuron to light up the ""2"" at 100% (and everything else at 0%).  does this extreme goal perhaps cause really confident misclassifications sometimes (besides the junk image from last video)?  would it be in any way beneficial to train the network to see a ""2"", but perhaps to only be 90% confident about it?  maybe it would *think* it saw a 2, but it's not totally sure - like a young child learning numbers.  (but, being 90% a 2, is still a 2.)  would this help at all with the junk images from the previous question / video?  also, maybe since we train the network to not be so confident all the time, could we then put any image that falls below, say, 75% confidence into another bin for humans to verify?  also, on bb&b you mentioned that it can sometimes feel like too much pressure when people say ""yeah, good video, i can't wait for the next one!""  well, i'll instead say, thanks for this series, and i'll patiently await the next video as you take your time to make it a high quality production like you always do - whenever that happens, i'm good.  no pressure!",True
@13thxenos,2017-11-03T14:45:03Z,13,"nicely done video.  i knew i learned backpropagation before, but it was hard, and i didn't use it manually ( i used frameworks like tensorflow which uses computational graphs and backpropagate automatically) so i've forgotten how it actually worked. but this video is a great resource for newcomers to anns and people like me that have forgotten the theory behind it all. thank you.",True
@SocialNomad,2017-11-03T14:31:00Z,5,how many chapters will there be in total?,True
@starrystarry6128,2017-11-03T14:27:59Z,117,"i'm understanding literally nothing, why am i watching this",True
@patelnirmal4726,2017-11-03T14:21:06Z,289,awesome channel,True
@3blue1brown,2017-11-03T14:20:15Z,1766,"two things worth adding here:   1) in other resources and in implementations, you'd typically see these formulas in some more compact vectorized form, which carries with it the extra mental burden to parse the hadamard product and to think through why the transpose of the weight matrix is used, but the underlying substance is all the same.  2) backpropagation is really one instance of a more general technique called ""reverse mode differentiation"" to compute derivatives of functions represented in some kind of directed graph form.",True